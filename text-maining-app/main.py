from __future__ import annotations

import io
import os
import re
import tempfile
from collections import Counter
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd
import streamlit as st
from wordcloud import STOPWORDS, WordCloud

try:
    from janome.tokenizer import Tokenizer
except Exception:  # pragma: no cover - handled by dependency setup
    Tokenizer = None


JAPANESE_STOPWORDS = {
    "ã“ã‚Œ",
    "ãã‚Œ",
    "ã‚ã‚Œ",
    "ã“ã®",
    "ãã®",
    "ã‚ã®",
    "ãŸã‚",
    "ã‚ˆã†",
    "ã“ã¨",
    "ã‚‚ã®",
    "ã¨ã“ã‚",
    "ã¨ã",
    "ã•ã‚“",
    "ã™ã‚‹",
    "ã„ã‚‹",
    "ãªã‚‹",
    "ã‚ã‚‹",
    "ã¾ã™",
    "ã§ã™",
    "ã§ãã‚‹",
}

DEFAULT_SAMPLE = """æ˜¨æ—¥ã®å®šä¾‹ä¼šè­°ã§ã¯ã€æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é€²ã‚æ–¹ã‚’è­°è«–ã—ã¾ã—ãŸã€‚
ç‰¹ã«ã€è¦ä»¶å®šç¾©ã®ç²¾åº¦ã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ãŒé‡è¦ã ã¨ã„ã†æ„è¦‹ãŒå¤šã‹ã£ãŸã§ã™ã€‚
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä¼šè­°ã®ã‚³ãƒ¡ãƒ³ãƒˆã§ã¯ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«èª¿æ•´ã¨ãƒªã‚¹ã‚¯ç®¡ç†ã«é–¢ã™ã‚‹ç™ºè¨€ãŒç›®ç«‹ã¡ã¾ã—ãŸã€‚"""

LANG_AUTO = "Auto"
LANG_JA = "Japanese"
LANG_EN = "English"


def contains_japanese(text: str) -> bool:
    return bool(re.search(r"[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]", text))


def tokenize_english(
    text: str,
    min_len: int,
    include_numbers: bool,
    stopwords: set[str],
) -> list[str]:
    if include_numbers:
        pattern = r"[A-Za-z0-9][A-Za-z0-9'\-]*"
    else:
        pattern = r"[A-Za-z][A-Za-z'\-]*"
    words = re.findall(pattern, text.lower())
    return [word for word in words if len(word) >= min_len and word not in stopwords]


def tokenize_japanese(
    text: str,
    min_len: int,
    include_pos: set[str],
    stopwords: set[str],
) -> list[str]:
    if Tokenizer is None:
        return []
    tokenizer = Tokenizer()
    tokens: list[str] = []
    for token in tokenizer.tokenize(text):
        pos = token.part_of_speech.split(",")[0]
        base = token.base_form
        if base == "*":
            base = token.surface
        if pos not in include_pos:
            continue
        if len(base) < min_len:
            continue
        if base in stopwords:
            continue
        tokens.append(base)
    return tokens


def find_font_path(uploaded_font: bytes | None, filename: str | None) -> str | None:
    if uploaded_font and filename:
        suffix = Path(filename).suffix or ".ttf"
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(uploaded_font)
            return tmp.name

    candidate_paths = [
        "C:/Windows/Fonts/meiryo.ttc",
        "C:/Windows/Fonts/msgothic.ttc",
        "/usr/share/fonts/truetype/noto/NotoSansCJKjp-Regular.otf",
        "/usr/share/fonts/opentype/noto/NotoSansCJKjp-Regular.otf",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
        "/Library/Fonts/Arial Unicode.ttf",
    ]
    for path in candidate_paths:
        if os.path.exists(path):
            return path
    return None


def build_wordcloud(
    frequencies: dict[str, int],
    width: int,
    height: int,
    background_color: str,
    colormap: str,
    font_path: str | None,
) -> WordCloud:
    return WordCloud(
        width=width,
        height=height,
        background_color=background_color,
        colormap=colormap,
        font_path=font_path,
        stopwords=None,
    ).generate_from_frequencies(frequencies)


def main() -> None:
    st.set_page_config(
        page_title="AI Text Mining Word Cloud",
        page_icon="ğŸ§ ",
        layout="wide",
    )

    st.title("AI Text Mining Word Cloud")
    st.write(
        "è­°äº‹éŒ²ã‚„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä¼šè­°ã‚³ãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›ã™ã‚‹ã¨ã€é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚"
    )

    with st.sidebar:
        st.header("Settings")
        language = st.selectbox("Language", [LANG_AUTO, LANG_JA, LANG_EN], index=0)
        min_len = st.slider("Minimum Word Length", 1, 6, 2)
        max_words = st.slider("Max Words", 20, 200, 80)
        include_numbers = st.checkbox("Include Numbers (English)", value=False)
        include_pos = st.multiselect(
            "Japanese POS",
            ["åè©", "å½¢å®¹è©", "å‹•è©"],
            default=["åè©", "å½¢å®¹è©"],
        )
        background = st.selectbox("Background", ["white", "black"])
        colormap = st.selectbox(
            "Colormap",
            ["viridis", "plasma", "inferno", "magma", "cividis", "Set2", "tab20"],
        )
        uploaded_font = st.file_uploader("Font File (optional)", type=["ttf", "otf", "ttc"])

    text = st.text_area("Input Text", height=240, value=DEFAULT_SAMPLE)

    if st.button("Analyze", type="primary"):
        if not text.strip():
            st.error("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        if language == LANG_AUTO:
            resolved_language = LANG_JA if contains_japanese(text) else LANG_EN
        else:
            resolved_language = language

        font_path = find_font_path(
            uploaded_font.getvalue() if uploaded_font else None,
            uploaded_font.name if uploaded_font else None,
        )

        if resolved_language == LANG_JA:
            if Tokenizer is None:
                st.error("æ—¥æœ¬èªè§£æã«ã¯ janome ãŒå¿…è¦ã§ã™ã€‚ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                return
            tokens = tokenize_japanese(
                text,
                min_len=min_len,
                include_pos=set(include_pos),
                stopwords=JAPANESE_STOPWORDS,
            )
        else:
            tokens = tokenize_english(
                text,
                min_len=min_len,
                include_numbers=include_numbers,
                stopwords=set(word.lower() for word in STOPWORDS),
            )

        if not tokens:
            st.warning("æŠ½å‡ºã•ã‚ŒãŸå˜èªãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
            return

        counts = Counter(tokens)
        top_counts = counts.most_common(max_words)
        frequencies = dict(top_counts)

        wordcloud = build_wordcloud(
            frequencies=frequencies,
            width=1000,
            height=500,
            background_color=background,
            colormap=colormap,
            font_path=font_path,
        )

        col1, col2 = st.columns([2, 1], gap="large")
        with col1:
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wordcloud, interpolation="bilinear")
            ax.axis("off")
            st.pyplot(fig, use_container_width=True)

            buffer = io.BytesIO()
            fig.savefig(buffer, format="png", bbox_inches="tight", dpi=150)
            st.download_button(
                "Download Word Cloud (PNG)",
                data=buffer.getvalue(),
                file_name="wordcloud.png",
                mime="image/png",
            )

        with col2:
            st.subheader("Top Keywords")
            df = pd.DataFrame(top_counts, columns=["keyword", "count"])
            st.dataframe(df, use_container_width=True, height=420)
            st.download_button(
                "Download Keywords (CSV)",
                data=df.to_csv(index=False).encode("utf-8"),
                file_name="keywords.csv",
                mime="text/csv",
            )

        if resolved_language == LANG_JA and font_path is None:
            st.info(
                "æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€æ–‡å­—ãŒå››è§’ãè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚"
                "å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
            )


if __name__ == "__main__":
    main()
